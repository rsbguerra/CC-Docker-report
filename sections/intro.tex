\section{Introduction}
\label{sec::intro}

For a long period, ensuring that an application's performance was consistent across multiple \acsp{OS} --- while managing different versions of said \acsp{OS}, programming languages and libraries --- was almost an impossible task. Thus, IT departments began to adopt the use of virtualization. \acsp{VM} have solved multiple issues regarding portability and compatibility of applications, as well as introducing the concept of application isolation. They allowed multiple applications to be executed in parallel on the same machine, without the risk of interference between them, by creating multiple \acsp{VM} on the same system.

However, virtualization has several drawbacks, the biggest one being the performance degradation due to the heavy nature of an entire \acs{OS} virtualization and even different architectures. In addition, the need for portability was not fully met with \acsp{VM}. Hypervisor software from different vendors creates an additional roadblock to deploying applications regardless of the underlying hardware.

Therefore, the IT industry has been steadily adopting the use of containerization. Containers provide a lightweight and portable solution including all the libraries, binaries and other dependencies a given application needs to be deployed without affecting the host's performance.

Despite being similar regarding the provision of some level of isolation from the host's system in order to deploy applications in a greater range of machines, containers and \acsp{VM} achieve this in a widely different manner.

Containers depend on Linux kernel functionalities to create a thin layer of isolation between the native \acs{OS} and the application. As such, resource sharing is done with greater efficiency, thus maintaining almost bare-metal performance. \acsp{VM} are not able to provide this level of efficiency, but they do achieve a greater level of isolation by emulating both hardware and \acs{OS}. Despite not providing the same level of isolation, and consequently security, the containerization paradigm is an increasingly appealing option. Hence, Docker enters the picture.

The Docker initiative was designed specifically to ease the implementation and automation of container creation. As such, Docker is a software that creates, manages and orchestrates containers. It is part of the Moby project, an open-source project available on GitHub \cite{moby-git}. Since its initial release in 2013, its ease of application deployment has continually revolutionized the way software is delivered and run.

Docker's rapid development and adoption created the need of standardization regarding containers and images creation. Therefore, a council was founded with the goal of governing the containerization standards: the \ac{OCI}.

Given the rapid and unanimous adoption of Docker by the industry, it is relevant to explore this software and understand how it works in order to fully take advantage of the containerization features Docker provides. Thus, this paper is not intended to be a tutorial on how to use Docker and the features that will be explored, but as an overview on how Docker manages containerization; this includes both the overall architecture and the main features that provides secure, reliable and robust isolation. With this paper the author intends to pull back the abstraction Docker provides and present how Docker automates and simplifies the creation of secure containers by taking advantage of the tools made available by the Linux kernel. 

As such, it is important to note that only the Linux distribution of Docker will be discussed. As Docker resorts frequently to Linux libraries and kernel features, it would be out of this paper's scope to discuss how these features are translated to the Windows environment. As such, many of the nomenclature regarding various Docker modules or drivers referenced in this paper will differ from the Windows implementation.