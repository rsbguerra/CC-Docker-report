\section{Docker Architecture}
\label{sec::arch}

\placeholder{Architecture intro}

% 3. Docker Architecture
%     1. The Docker engine
%         1. Client server
%         2. Monolithic architecture
%         3. runc
%         4. containerd
%     2. Images
%         1. Layers
%         2. Dockerfile
%         3. Docker compose
%     3. Containers
%         1. Container creation process
%         2. Workflow
%     5. Volumes and persistent data
%     6. Networking

% TODO:
% explain libcontainer

\subsection{The Docker Engine}
\label{sec::arch:engine}
The Docker Engine is the core software responsible for running and managing containers, which consists of several modular components\cite{Docker-engine}. It is implemented as a client-server application, consisting of:

\begin{itemize}
    \item A server running the daemon process \texttt{dockerd};
    \item Docker's \acs{REST}ful \acs{API};
    \item The \acs{CLI} client, \texttt{docker}.
\end{itemize}

Within this architecture, the Docker daemon is the component responsible for creating, running and managing the containers. The daemon is able to manage any number of containers in the device, all controlled via the \acs{CLI} client. The client interacts with the Docker daemon via its own \acs{REST} \acs{API} over Unix sockets or a network interface\cite{Poulton2020-ju}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.25\textwidth]{docker_engine.png}
    \caption{Current and planned container use for all organizations.}
    \label{fig::docker-engine-server}
\end{figure}

Despite being, at its core, a client-server architecture, Docker's server is far from monolithic, as shown in figure \ref{fig:docker-engine-server}. The server operates a set of smaller components, among them \texttt{runc} and \texttt{containerd}, the main modules responsible for creating and running containers. Both implement as closely as possible the OCI open source project specifications\cite{Poulton2020-ju, oci-runc, git-runc, docker-containerd}.

The first program, \texttt{runc}, is a \acs{CLI} component responsible for creating and running applications. Its purpose is to be a standalone low-level tool called by any high-level application, such as Docker. In order to achieve this, \texttt{runc} relies on the library \texttt{libcontainer} to access the container isolation features of the \acs{OS}, such as \texttt{seccomp} and user namespaces\cite{runc-estes}.

Secondly, \texttt{containerd} is responsible for managing all the container execution logic, such as starting, stopping and removing containers. In the same way as \texttt{runc}, \texttt{containerd} is available as a standalone daemon and was intended to be a lightweight program with the sole role of managing container lifetime operations. But as it grew, it gained new functionalities, such as data storage and image management\cite{docker-containerd}.

With this in mind, it's possible to understand the container creation workflow. When a command is typed in the \acs{CLI}, the Docker client converts the instructions into the \acs{API} payload and POSTs to the correct API endpoint. Once the daemon receives the command to create a new container, a call is made to \texttt{containerd}, which converts the required Docker image into a bundle and instructs \texttt{runc} to create a new container. In its turn, \texttt{runc} interfaces with the \acs{OS} kernel to allocate the resources and to execute the instructions needed to create a new container. The container process is started as a child-process of \texttt{runc}, and as soon as it is started, \texttt{runc} is terminated.


\subsection{Containers}
\label{sec::arch:containers}
As defined by the \texttt{libcontainer} README page, a container is "a self-contained execution environment that shares the kernel of the host system and is optionally isolated from other containers in the system"\cite{docker-libcontainer}.

Despite the behavior similarities between containers and \acsp{VM}, they are fundamentally different since all the containers running in the system share the same kernel, so the isolation between workloads is implemented within that one kernel. As a result, there is one less layer of abstraction between the isolated task and the hardware underneath, which results in better resource efficiency.

However, it's only possible to run processes that are compatible with the underlining kernel, \ie~ a Windows container cannot run Linux application and vice versa.

% \subsection{Container Lifecycle}
% \placeholder{Dockerfile}
% 
% \placeholder{docker compose}
% containers follow the same process group signal propagations that any other process group would recieve on linux
% auto restarting a continer?
% normal docker stop sends a SIGTERM to kill the container
% docker stop initially sends sigterm signal, if the container as not stopped within 25 seconds, a SIGKILL signal is sent to forcefully kill it
% if docker kill wont stop the container, docker kill is used

% The Dockerfile has two main purposes:
% 1. To describe the application
% 2. To tell Docker how to containerize the application (create an image with the app inside)

\subsection{Images}
\label{sec::arch:images}
Docker images are a template of what is reconstructed into a running container, it serves as a base for everything that will be deployed and run on the container, similarly to a filesystem \cite{Kane2018-fn}. Before launching a container, the image must be built from scratch or downloaded from a public registry. When the \texttt{docker pull} command is used, the default registry used is the official Docker Hub \cite{docker-hub}.

An image consists of multiple read-only layers; each layer represents the changes made to the underlining system, \ie~ a \texttt{diff} of the previous layer \cite{images-layers}. When a container is created, a new read-write layer is added on top of the previous ones, so that all the changes made during the container execution are saved on disk (figure \ref{fig:docker-image}) \cite{fig-src:image-layers}.

A layer is identified by a digest, referred to as \textit{Content Addressable IDs}, as each hash represents the image content. This model of layer separation allows several images to address the same layer. As such, the latter only needs to be stored once in the device.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.45\textwidth]{container-layers.jpg}
    \caption{Diagram exemplificative of a Docker image composition, where the set of layers composes an Ubuntu container \cite{fig-src:image-layers}.}
    \label{fig:docker-image}
\end{figure}

These layers are described in the image manifests, which are \acs{JSON} files containing both information about the image's layers and a list of compatible architectures given by an image tag \cite{image-manifest}.

\placeholder{Use image storage driver to connect to next section}
% https://docs.docker.com/storage/storagedriver/

\subsection{Volumes and persistent data}
\label{sec::arch:volumes}

When a container is created, the system automatically allocates local storage where the container's filesystem is stored. However, this integral part of the container is tied to it's lifecycle, it will be deleted as soon the container itself is deleted. By default, the container uses only this local storage. If persistent storage is needed, there are two options for containers to store data on the host device: volumes and bind mounts\cite{container-storage}.

Volumes are created and managed by Docker and are stored within a folder on the host machine. As volumes are only managed by Docker, this means they are isolated from the host and only Docker is allowed to write any change made to the volume. It's possible to mount a single volume into multiple containers. When a given volume isn't being used by any container it won't be removed automatically.

On the other hand, bind mounts consist on directories on the host machine being mounted into a container. This mounts are limited in functionality, as they are not managed by Docker itself.

\placeholder{Connect section to Networking, if possible}

\subsection{Networking}
\label{ssec::arch:net}

Different types of bridge networking:

\begin{itemize}
    \item bridge
    \item host
    \item overlay
    \item macvlan
\end{itemize}

