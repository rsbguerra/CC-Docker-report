\section{Docker Architecture}
\label{sec::arch}

\placeholder{Architecture intro}

% 3. Docker Architecture
%     1. The Docker engine
%         1. Client server
%         2. Monolithic architecture
%         3. runc
%         4. containerd
%     2. Images
%         1. Layers
%         2. Dockerfile
%         3. Docker compose
%     3. Containers
%         1. Container creation process
%         2. Workflow
%     5. Volumes and persistent data
%     6. Networking

% TODO:
% explain libcontainer

\subsection{The Docker Engine}
\label{sec::arch:engine}
The Docker Engine is the core software responsible for running and managing containers, which consists of several modular components\cite{Docker-engine}. It is implemented as a client-server application, consisting of:

\begin{itemize}
    \item A server running the daemon process \texttt{dockerd};
    \item Docker's \acs{REST}ful \acs{API};
    \item The \acs{CLI} client, \texttt{docker}.
\end{itemize}

Within this architecture, the Docker daemon is the component responsible for creating, running and managing the containers. The daemon is able to manage any number of containers in the device, all controlled via the \acs{CLI} client. The client interacts with the Docker daemon via its own \acs{REST} \acs{API} over Unix sockets or a network interface\cite{Poulton2020-ju}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.25\textwidth]{docker_engine.png}
    \caption{\placeholder{Current and planned container use for all organizations.}}
    \label{fig::docker-engine-server}
\end{figure}

Despite being, at its core, a client-server architecture, Docker's server is far from monolithic, \placeholder{as shown in figure \ref{fig::docker-engine-server}}. The server operates a set of smaller components, among them \texttt{runc} and \texttt{containerd}, the main modules responsible for creating and running containers. Both implement as closely as possible the OCI open source project specifications\cite{Poulton2020-ju, oci-runc, git-runc, docker-containerd}.

The first program, \texttt{runc}, is a \acs{CLI} component responsible for creating and running applications. Its purpose is to be a standalone low-level tool called by any high-level application, such as Docker. In order to achieve this, \texttt{runc} relies on the library \texttt{libcontainer} to access the container isolation features of the \acs{OS}, such as \texttt{seccomp} and user namespaces\cite{runc-estes}.

Secondly, \texttt{containerd} is responsible for managing all the container execution logic, such as starting, stopping and removing containers. In the same way as \texttt{runc}, \texttt{containerd} is available as a standalone daemon and was intended to be a lightweight program with the sole role of managing container lifetime operations. But as it grew, it gained new functionalities, such as data storage and image management\cite{docker-containerd}.

With this in mind, it's possible to understand the container creation workflow. When a command is typed in the \acs{CLI}, the Docker client converts the instructions into the \acs{API} payload and POSTs to the correct API endpoint. Once the daemon receives the command to create a new container, a call is made to \texttt{containerd}, which converts the required Docker image into a bundle and instructs \texttt{runc} to create a new container. In its turn, \texttt{runc} interfaces with the \acs{OS} kernel to allocate the resources and to execute the instructions needed to create a new container. The container process is started as a child-process of \texttt{runc}, and as soon as it is started, \texttt{runc} is terminated.



\subsection{Containers}
\label{sec::arch:containers}
As defined by the \texttt{libcontainer} README page, a container is "a self-contained execution environment that shares the kernel of the host system and is optionally isolated from other containers in the system"\cite{docker-libcontainer}.

Despite the behavior similarities between containers and \acsp{VM}, they are fundamentally different since all the containers running in the system share the same kernel, so the isolation between workloads is implemented within that one kernel. As a result, there is one less layer of abstraction between the isolated task and the hardware underneath, which results in better resource efficiency.

However, it's only possible to run processes that are compatible with the underlining kernel, \ie~ a Windows container cannot run Linux application and vice versa.

% containers follow the same process group signal propagations that any other process group would recieve on linux
% auto restarting a continer?
% normal docker stop sends a SIGTERM to kill the container
% docker stop initially sends sigterm signal, if the container as not stopped within 25 seconds, a SIGKILL signal is sent to forcefully kill it
% if docker kill wont stop the container, docker kill is used



\subsection{Images}
\label{sec::arch:images}
Docker images are a template of what is reconstructed into a running container, it serves as a base for everything that will be deployed and run on the container, similarly to a filesystem \cite{Kane2018-fn}. Before launching a container, the image must be built from scratch or downloaded from a public registry. When the \texttt{docker pull} command is used, the default registry used is the official Docker Hub \cite{docker-hub}.

An image consists of multiple read-only layers; each layer represents the changes made to the underlining system, \ie~ a \texttt{diff} of the previous layer \cite{images-layers}. When a container is created, a new read-write layer is added on top of the previous ones, so that all the changes made during the container execution are saved on disk (figure \ref{fig:docker-image}) \cite{fig-src:image-layers}.

A layer is identified by a digest, referred to as \textit{Content Addressable IDs}, as each hash represents the image content. This model of layer separation allows several images to address the same layer. As such, the latter only needs to be stored once in the device.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.45\textwidth]{container-layers.jpg}
    \caption{Diagram exemplificative of a Docker image composition, where the set of layers composes an Ubuntu container \cite{fig-src:image-layers}.}
    \label{fig:docker-image}
\end{figure}

These layers are described in the image manifests, which are \acs{JSON} files containing both information about the image's layers and a list of compatible architectures given by an image tag \cite{image-manifest}.

\placeholder{Use image storage driver to connect to next section}
% https://docs.docker.com/storage/storagedriver/




\subsection{Volumes and persistent data}
\label{sec::arch:volumes}

When a container is created, the system automatically allocates local storage where the container's filesystem is stored. However, this integral part of the container is tied to it's lifecycle, it will be deleted as soon the container itself is deleted. By default, the container uses only this local storage. If persistent storage is needed, there are two options for containers to store data on the host device: volumes and bind mounts\cite{container-storage}.

Volumes are created and managed by Docker and are stored within a folder on the host machine. As volumes are only managed by Docker, this means they are isolated from the host and only Docker is allowed to write any change made to the volume. It's possible to mount a single volume into multiple containers. When a given volume isn't being used by any container it won't be removed automatically.

On the other hand, bind mounts consist on directories on the host machine being mounted into a container. This mounts are limited in functionality, as they are not managed by Docker itself.

\placeholder{Connect section to Networking, if possible}



\subsection{Networking}
\label{ssec::arch:net}
\placeholder{Begin draft of network}

A container attached to a Docker network will get a unique IP address

Docker also treats networks as first-class entities, own life cycle and are not bound to any other object

At the highest level, Docker networking comprises three major components:

\begin{itemize}
    \item The Container Network Model (CNM) -- design specification. outlines the fundamental building blocks of a Docker network.
    \item libnetwork -- real-world implementation of the CNM, and is used by Docker. It's written in Go, and implements the core components outlined in the CNM.
    \item Drivers
\end{itemize}

\placeholder{Seperate paragraph about CNM}

A \textbf{sandbox} is an isolated network stack. It includes; Ethernet interfaces, ports, routing tables, and DNS config.

\textbf{Endpoints} are virtual network interfaces (E.g. veth). Like normal network interfaces, they're responsible for making connections. In the case of the CNM, it's the job of the endpoint to connect a sandbox to a network.

\textbf{Networks} are a software implementation of an 802.1d bridge (more commonly known as a switch). As such, they group together, and isolate, a collection of endpoints that need to communicate.

Libnetwork is The CNM is the design doc, and libnetwork is the canonical implementation. It's open-source, written in Go, cross-platform (Linux and Windows), and used by Docker.

Different types of network drivers:

\begin{itemize}
    \item \textbf{bridge} - The bridge driver provides intercontainer connectivity for all containers running on the same machine. best when you need multiple containers to communicate on the same Docker host.
    \item \textbf{host} - The host network is provided by the host driver, which instructs Docker not to create any special networking namespace or resources for attached containers. Containers on the host network interact with the host's network stack like uncontained processes. best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated.
    \item \textbf{none} - the none network uses the null driver. Containers attached to the network will not have any network connectivity outside themselves.
    \item \textbf{overlay} - best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using swarm services.
    \item \textbf{macvlan} - The overlay network driver creates a distributed network among multiple Docker daemon hosts. This network sits on top of (overlays) the host-specific networks, allowing containers connected to it (including swarm service containers) to communicate securely when encryption is enabled. Some applications, especially legacy applications or applications which monitor network traffic, expect to be directly connected to the physical network. In this type of situation, you can use the macvlan network driver to assign a MAC address to each container's virtual network interface, making it appear to be a physical network interface directly connected to the physical network. best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address.
\end{itemize}

The scope of a network can take three values: local, global, or swarm.

This indicates whether the network is constrained to the machine where the network exists (local), should be created on every node in a cluster but not route between them (global), or seamlessly spans all of the hosts participating in a Docker swarm (multi-host or cluster-wide).

the default networks have the local scope, and will not be able to directly route traffic between containers running on different machines.

\placeholder{End draft of network}
